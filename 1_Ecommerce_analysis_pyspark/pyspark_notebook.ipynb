{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Instructions\n",
    "\n",
    "\n",
    "### 1/ Description\n",
    "\n",
    "The goal here is to get insights from a dataset by using Spark, and write the analysis and additional information into a brief report.\n",
    "\n",
    "### 2/ Contents Organization\n",
    "\n",
    "The content of the attached report is as follows:\n",
    "- Background/scenario description (what)\n",
    "- Goal of the analysis (why)\n",
    "- Analysis deep dive (how)\n",
    "- Conclusions (insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PySpark environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init() \n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data source and Spark data abstraction (DataFrame) setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecomDF = spark.read \\\n",
    "                 .option(\"inferSchema\", \"true\") \\\n",
    "                 .option(\"header\", \"true\") \\\n",
    "                 .csv(\"ecommerce.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data set metadata analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. An Introduction to the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from the UCI Machine Learning Repository that has made this rare e-commerce dataset publicly available. It can be found here: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II.\n",
    "\n",
    "##### Description: \n",
    "This dataset contains all the transactionsfor an Online Retail business registered in the United Kingdom. The transactions took place from the 1st of December 2009 to the 9th of December 2011. The main products sold by the company are unique all-occasion gift-ware. And also, many customers of this e-commerce are wholesalers.\n",
    "\n",
    "\n",
    "##### The variables information: \n",
    "\n",
    "**InvoiceNo**: a nominal Invoice number. It is a 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n",
    "\n",
    "**StockCode**: the nominal product (item) code. A 5-digit integral number uniquely assigned to each distinct product.\n",
    "\n",
    "**Description**: the nominal product (item) name. \n",
    "\n",
    "**Quantity**: the quantities of each product (item) per transaction (numeric).\n",
    "\n",
    "**InvoiceDate**: the invoice date and time (numeric). The day and time when a transaction was generated.\n",
    "\n",
    "**UnitPrice**: the unit price (numeric). Product price per unit (in sterling pound).\n",
    "\n",
    "**CustomerID**: a nominal customer number. A 5-digit integral number uniquely assigned to each customer.\n",
    "\n",
    "**Country**: the nominal country name. The name of the country where a customer resides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Display schema and size of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This DataFrame has **541909 rows**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "ecomDF.printSchema() \n",
    "display(Markdown(\"This DataFrame has **%d rows**.\" % ecomDF.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Get one or multiple random samples from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='536367', StockCode='21755', Description='LOVE BUILDING BLOCK WORD', Quantity=3, InvoiceDate='12/1/2010 8:34', UnitPrice=5.95, CustomerID=13047, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536370', StockCode='22544', Description='MINI JIGSAW SPACEBOY', Quantity=24, InvoiceDate='12/1/2010 8:45', UnitPrice=0.42, CustomerID=12583, Country='France'),\n",
       " Row(InvoiceNo='536375', StockCode='84029G', Description='KNITTED UNION FLAG HOT WATER BOTTLE', Quantity=6, InvoiceDate='12/1/2010 9:32', UnitPrice=3.39, CustomerID=17850, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536378', StockCode='22386', Description='JUMBO BAG PINK POLKADOT', Quantity=10, InvoiceDate='12/1/2010 9:37', UnitPrice=1.95, CustomerID=14688, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536378', StockCode='21977', Description='PACK OF 60 PINK PAISLEY CAKE CASES', Quantity=24, InvoiceDate='12/1/2010 9:37', UnitPrice=0.55, CustomerID=14688, Country='United Kingdom')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomDF.cache()\n",
    "ecomDF.sample(False, 0.1).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Data entities, metrics and dimensions\n",
    "\n",
    "I've identified the following elements:\n",
    "\n",
    "* **Entities:** Sales transactions (main one measured - facts), InvoiceNo (dimension), CustomerID (dimension)\n",
    "* **Metrics:** Quantity, InvoiceDate, UnitPrice\n",
    "* **Dimensions:** StockCode, Description, Country\n",
    "\n",
    "### E. Column categorization\n",
    "\n",
    "The following could be a potential column categorization:\n",
    "\n",
    "* **Timing related columns:** *InvoiceDate*\n",
    "* **Sales related columns:** *InvoiceNo*, *StockCode*, *Description*, *Quantity*, *UnitPrice*\n",
    "* **Customer related columns:** *CustomerID*, *Country*\n",
    "\n",
    "(the data will be enriched before jumping to Data Profiling, and I will add more Timing related columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Data Expansion\n",
    "\n",
    "Before the data profiling, I believe it is appropriate to perform some expansion of our data, solely based on the columns we have. These expansions will mainly be on the time column. I would like to expand it to be able to perform better analysis later on. I thus expand the original InvoiceDate column into: date, month, week, day, hour, period of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+---------+----+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|     Date|Time|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+---------+----+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|12/1/2010|8:26|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|12/1/2010|8:26|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+---------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "Checking for nulls on columns Date and Time as compared to InvoiceDate:\n",
      "+----+----+-----------+\n",
      "|Date|Time|InvoiceDate|\n",
      "+----+----+-----------+\n",
      "|   0|   0|          0|\n",
      "+----+----+-----------+\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, count, when, col\n",
    "\n",
    "split_col = split(ecomDF['InvoiceDate'], ' ')\n",
    "ecomDF = ecomDF.withColumn('Date', split_col.getItem(0))\n",
    "ecomDF = ecomDF.withColumn('Time', split_col.getItem(1))\n",
    "\n",
    "#checking if the column were created:\n",
    "ecomDF.show(2)\n",
    "\n",
    "#checking if they are correctly filled:\n",
    "print(\"Checking for nulls on columns Date and Time as compared to InvoiceDate:\")\n",
    "ecomDF.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"Date\",\"Time\", \"InvoiceDate\"]]).show()\n",
    "\n",
    "ecomDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----------+----------+----+---------+---------+--------------------+--------+---------+----------+--------------+\n",
      "|  DateFull|year|month|day|DayofWeekN|DayofWeekS|Time|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|\n",
      "+----------+----+-----+---+----------+----------+----+---------+---------+--------------------+--------+---------+----------+--------------+\n",
      "|2010-12-01|2010|   12|  1|         3|       Wed|8:26|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|\n",
      "|2010-12-01|2010|   12|  1|         3|       Wed|8:26|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|\n",
      "|2010-12-01|2010|   12|  1|         3|       Wed|8:26|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|\n",
      "|2010-12-01|2010|   12|  1|         3|       Wed|8:26|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|\n",
      "|2010-12-01|2010|   12|  1|         3|       Wed|8:26|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|\n",
      "+----------+----+-----+---+----------+----------+----+---------+---------+--------------------+--------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DateFull: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- DayofWeekN: integer (nullable = true)\n",
      " |-- DayofWeekS: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we see that the type of Date is a string, so to be able to add the column \"DayOfWeek\", we have to transform it to a Date type and apply the function\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf, dayofweek, date_format, year, month, dayofmonth\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "\n",
    "# This function converts the string cell into a date:\n",
    "func =  udf (lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())\n",
    "\n",
    "ecomDF1 = ecomDF.withColumn('DateFull', func(col('Date')))\n",
    "\n",
    "#We want to add to our main dataframe a column containing the day of the week:\n",
    "ecomDF1=ecomDF1.select('DateFull' , \"Time\",\"InvoiceNo\",\"StockCode\", \"Description\",\"Quantity\",\"UnitPrice\",\"CustomerID\",\"Country\",\n",
    "               date_format('Datefull', 'u').alias('DayOfWeekN'), date_format('DateFull', 'E').alias('DayOfWeekS'))\n",
    "\n",
    "\n",
    "#I also add the year, month, and day numbers coming from the date:\n",
    "ecomDF1= ecomDF1.select('DateFull' , \"Time\",\"InvoiceNo\",\"StockCode\", \"Description\",\"Quantity\",\"UnitPrice\",\n",
    "                        \"CustomerID\",\"Country\", \"DayofWeekN\", \"DayofWeekS\",\n",
    "    year(\"DateFull\").alias('year'), \n",
    "    month(\"DateFull\").alias('month'), \n",
    "    dayofmonth(\"DateFull\").alias('day'))\n",
    "\n",
    "#I reorder the columns to have the time related ones next to each other:\n",
    "ecomDF1=ecomDF1.select(\"DateFull\" ,\"year\",\"month\",\"day\",\"DayofWeekN\", \"DayofWeekS\", \"Time\",\"InvoiceNo\",\"StockCode\", \"Description\",\"Quantity\",\"UnitPrice\",\n",
    "                        \"CustomerID\",\"Country\")\n",
    "\n",
    "#and finally changing the DafofWeekN into an integer because it was given as a string type:\n",
    "ecomDF1 = ecomDF1.withColumn(\"DayofWeekN\", ecomDF1[\"DayofWeekN\"].cast(IntegerType()))\n",
    "ecomDF1.show(5)\n",
    "ecomDF1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Columns groups basic profiling to better understand our data set\n",
    "### A. Timing related columns basic profiling\n",
    "\n",
    "We run profiling on the time related columns to see if there are any irregularities we could find:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the summary of the columns year, month, and DayofWeekN:\n",
      "+-------+-------------------+-----------------+------------------+------------------+\n",
      "|summary|               year|            month|               day|        DayofWeekN|\n",
      "+-------+-------------------+-----------------+------------------+------------------+\n",
      "|  count|             541909|           541909|            541909|            541909|\n",
      "|   mean| 2010.9216086095637|7.553127923691985|15.023096128685813|  3.43127720705875|\n",
      "| stddev|0.26878674384472484|3.509055367918596| 8.664062753327219|1.8447086898496354|\n",
      "|    min|               2010|                1|                 1|                 1|\n",
      "|    25%|               2011|                5|                 7|                 2|\n",
      "|    50%|               2011|                8|                15|                 3|\n",
      "|    75%|               2011|               11|                22|                 5|\n",
      "|    max|               2011|               12|                31|                 7|\n",
      "+-------+-------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown \n",
    "from pyspark.sql.functions import when, count, col, countDistinct, desc, first, lit \n",
    "\n",
    "\n",
    "print (\"Here are the summary of the columns year, month, and DayofWeekN:\")\n",
    "ecomDF1.select(\"year\",\"month\",\"day\",\"DayofWeekN\").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS:\n",
    "- We see that those 5 time-related columns behave normally\n",
    "- For now we can say that we have data for 2 years: 2010 and 2011\n",
    "- The month, day, and day of the week columns seem to be correct too\n",
    "\n",
    "We can now check for any nulls in these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now look for nulls in the columns year, month, and DayofWeekN:\n",
      "+----+-----+---+----------+\n",
      "|year|month|day|DayofWeekN|\n",
      "+----+-----+---+----------+\n",
      "|   0|    0|  0|         0|\n",
      "+----+-----+---+----------+\n",
      "\n",
      "And also for the distinct values in those columns:\n",
      "+----+-----+---+----------+\n",
      "|year|month|day|DayofWeekN|\n",
      "+----+-----+---+----------+\n",
      "|   2|   12| 31|         6|\n",
      "+----+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"We now look for nulls in the columns year, month, and DayofWeekN:\")\n",
    "ecomDF1.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"year\",\"month\",\"day\",\"DayofWeekN\"]]).show()\n",
    "\n",
    "print(\"And also for the distinct values in those columns:\")\n",
    "ecomDF1.select([countDistinct(c).alias(c) for c in [\"year\",\"month\",\"day\",\"DayofWeekN\"]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS:\n",
    "\n",
    "- All our columns are complete\n",
    "\n",
    "NOTE: here we do not expect to have nulls in all thse columns if the DateFull column has no nulls. Indeed, all these columns are derived from it and thus this is used here as a way to make sure that the transformation above were correctly implemented.\n",
    "\n",
    "We can now derive some basic insights that could help us in answering the main questions later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's look at the most and least frequent occurrences for the days in regards to month and week:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| leastFreqDayOfMonth | mostFreqDayOfMonth | leastFreqDayOfWeek | mostFreqDayOfWeek |\n",
       "|----|----|----|----|\n",
       "| 31 (10518 occurrences) | 8 (24658 occurrences) | 7 (64375 occurrences) | 4 (103857 occurrences) |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Let's look at the most and least frequent occurrences for the days in regards to month and week:\")\n",
    "dayofMonthOccurrencesDF = ecomDF1.groupBy(\"day\").agg(count(lit(1)).alias(\"Total\")) #grouping by day of month\n",
    "dayOfWeekDF = ecomDF1.groupBy(\"DayofWeekN\").agg(count(lit(1)).alias(\"Total\"))\n",
    "#calculate most and least occurences. Grouping by day of month and counting how many day of months appear in the dataset\n",
    "\n",
    "TheleastFreqDayOfMonth    = dayofMonthOccurrencesDF.orderBy(col(\"Total\").asc()).first()\n",
    "ThemostFreqDayOfMonth     = dayofMonthOccurrencesDF.orderBy(col(\"Total\").desc()).first()\n",
    "TheleastFreqDayOfWeek     = dayOfWeekDF.orderBy(col(\"Total\").asc()).first()\n",
    "ThemostFreqDayOfWeek      = dayOfWeekDF.orderBy(col(\"Total\").desc()).first()\n",
    "\n",
    "#displaying iy with the Markdown function:\n",
    "display(Markdown(\"\"\"\n",
    "| %s | %s | %s | %s |\n",
    "|----|----|----|----|\n",
    "| %s | %s | %s | %s |\n",
    "\"\"\" % (\"leastFreqDayOfMonth\", \"mostFreqDayOfMonth\", \"leastFreqDayOfWeek\", \"mostFreqDayOfWeek\", \\\n",
    "       \"%d (%d occurrences)\" % (TheleastFreqDayOfMonth[\"day\"], TheleastFreqDayOfMonth[\"Total\"]), \\\n",
    "       \"%d (%d occurrences)\" % (ThemostFreqDayOfMonth[\"day\"], ThemostFreqDayOfMonth[\"Total\"]), \\\n",
    "       \"%d (%d occurrences)\" % (TheleastFreqDayOfWeek[\"DayofWeekN\"], TheleastFreqDayOfWeek[\"Total\"]), \\\n",
    "       \"%d (%d occurrences)\" % (ThemostFreqDayOfWeek[\"DayofWeekN\"], ThemostFreqDayOfWeek[\"Total\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION:\n",
    "\n",
    "- We can directly conclude that the business has the least transactions on Sundays and the most on Thursdays\n",
    "- And they also have probably less transactions towards the end of the months as compared to the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Sales related columns basic profiling\n",
    "\n",
    "Let us now turn to the sales related columns and see what they tell us: *InvoiceNo*, *StockCode*, *Description*, *Quantity*, *UnitPrice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like above, we start with a summary of the columns. Here, we focus on the integer columns Quantity, and UnitPrice:\n",
      "+-------+------------------+-----------------+\n",
      "|summary|          Quantity|        UnitPrice|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|            541909|           541909|\n",
      "|   mean|  9.55224954743324|4.611113626083471|\n",
      "| stddev|218.08115785023355|96.75985306117803|\n",
      "|    min|            -80995|        -11062.06|\n",
      "|    25%|                 1|             1.25|\n",
      "|    50%|                 3|             2.08|\n",
      "|    75%|                10|             4.13|\n",
      "|    max|             80995|          38970.0|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql.functions import when, count, col, countDistinct, desc, first\n",
    "\n",
    "print (\"Like above, we start with a summary of the columns. Here, we focus on the integer columns Quantity, and UnitPrice:\")\n",
    "ecomDF1.select(\"Quantity\", \"UnitPrice\").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS:\n",
    "\n",
    "- It seems like we have some outliers in both columns. It is not very logical to have negative quantities and prices (or they could be returns by customers, this is to further analayse)\n",
    "- The maximum values too seem to be outliers, and maybe mistakes. It is not very logical to have an item's price at 38,970 when the average price of an item is around 4.6 and the median 4.13. \n",
    "\n",
    "We can now check for nulls and distinct values in the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Looking for nulls in the columns ,InvoiceNo, StockCode, Description, Quantity, and UnitPrice:\n",
      "+---------+---------+-----------+--------+---------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|UnitPrice|\n",
      "+---------+---------+-----------+--------+---------+\n",
      "|        0|        0|       1454|       0|        0|\n",
      "+---------+---------+-----------+--------+---------+\n",
      "\n",
      "And here the amount of distinct values in the columns:\n",
      "+---------+---------+-----------+--------+---------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|UnitPrice|\n",
      "+---------+---------+-----------+--------+---------+\n",
      "|    25900|     4070|       4223|     722|     1630|\n",
      "+---------+---------+-----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Now Looking for nulls in the columns ,InvoiceNo, StockCode, Description, Quantity, and UnitPrice:\")\n",
    "ecomDF1.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"UnitPrice\"]]).show()\n",
    "\n",
    "print(\"And here the amount of distinct values in the columns:\")\n",
    "ecomDF1.select([countDistinct(c).alias(c) for c in [\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"UnitPrice\"]]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS:\n",
    "\n",
    "- We have a few missing descriptions, but the number is minimal when compared to the total number of rows (541,909)\n",
    "- It seems that we have 25,900 sales transactions\n",
    "- It also seems that the company has a wide variety of products with 4,070 different codes\n",
    "\n",
    "We can now take a look at the most frequent item sold according to StockCode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's look at the most and least frequent occurrences for sales of products according to StockCode:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| leastFreqStockCode | mostFreqStockCode |\n",
       "|----|----|\n",
       "| 84899F (1 occurrences) | 85123A (2313 occurrences) |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Let's look at the most and least frequent occurrences for sales of products according to StockCode:\")\n",
    "StockCodeDF = ecomDF1.groupBy(\"StockCode\").agg(count(lit(1)).alias(\"Total\"))\n",
    "\n",
    "leastFreqStockCode    = StockCodeDF.orderBy(col(\"Total\").asc()).first()\n",
    "mostFreqStockCode     = StockCodeDF.orderBy(col(\"Total\").desc()).first()\n",
    "\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "| %s | %s |\n",
    "|----|----|\n",
    "| %s | %s |\n",
    "\"\"\" % (\"leastFreqStockCode\", \"mostFreqStockCode\", \\\n",
    "       \"%s (%d occurrences)\" % (leastFreqStockCode[\"StockCode\"], leastFreqStockCode[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (mostFreqStockCode[\"StockCode\"], mostFreqStockCode[\"Total\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Customer related columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the customer related columns and see what they tell us: *CustomerID*, *Country*.\n",
    "\n",
    "Let's first look at the nulls and distinct values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for nulls in columns CustomerID,and Country:\n",
      "+----------+-------+\n",
      "|CustomerID|Country|\n",
      "+----------+-------+\n",
      "|    135080|      0|\n",
      "+----------+-------+\n",
      "\n",
      "Checking amount of distinct values in columns CustomerID, and Country:\n",
      "+----------+-------+\n",
      "|CustomerID|Country|\n",
      "+----------+-------+\n",
      "|      4372|     38|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for nulls in columns CustomerID,and Country:\")\n",
    "ecomDF1.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"CustomerID\", \"Country\"]]).show()\n",
    "\n",
    "print(\"Checking amount of distinct values in columns CustomerID, and Country:\")\n",
    "ecomDF1.select([countDistinct(c).alias(c) for c in [\"CustomerID\", \"Country\"]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we encounter the first problem with the dataset, we have 135,080 transactions with a missing customer ID. This represents around 27% of the rows, which is a significant amount. \n",
    "\n",
    "For now we could keep them in the data for the analyses of the first elements.\n",
    "However, when answering questions about the customers, it would be reasonable to exclude these columns for the specific analysis.\n",
    "\n",
    "Other OBSERVATIONS:\n",
    "\n",
    "- The business has 4,372 registered customers coming from 38 countries\n",
    "\n",
    "Let's try analysing now the most and least frequent countries of purshase and customers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most and least frequent occurrences for Country, and CustomerID:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| leastFreqCountry | mostFreqCountry | leastFreqCustomerID | mostFreqCustomerID |\n",
       "|----|----|----|----|\n",
       "| Saudi Arabia (10 occurrences) | United Kingdom (495478 occurrences) | 15070 (1 occurrences) | None (135080 occurrences) |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Most and least frequent occurrences for Country, and CustomerID:\")\n",
    "CountryDF = ecomDF1.groupBy(\"Country\").agg(count(lit(1)).alias(\"Total\"))\n",
    "CustomerIDDF   = ecomDF1.groupBy(\"CustomerID\").agg(count(lit(1)).alias(\"Total\"))\n",
    "\n",
    "\n",
    "leastFreqCountry    = CountryDF.orderBy(col(\"Total\").asc()).first()\n",
    "mostFreqCountry     = CountryDF.orderBy(col(\"Total\").desc()).first()\n",
    "leastFreqCustomerID      = CustomerIDDF.orderBy(col(\"Total\").asc()).first()\n",
    "mostFreqCustomerID       = CustomerIDDF.orderBy(col(\"Total\").desc()).first()\n",
    "\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "| %s | %s | %s | %s |\n",
    "|----|----|----|----|\n",
    "| %s | %s | %s | %s |\n",
    "\"\"\" % (\"leastFreqCountry\", \"mostFreqCountry\", \"leastFreqCustomerID\", \"mostFreqCustomerID\", \\\n",
    "       \"%s (%d occurrences)\" % (leastFreqCountry[\"Country\"], leastFreqCountry[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (mostFreqCountry[\"Country\"], mostFreqCountry[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (leastFreqCustomerID[\"CustomerID\"], leastFreqCustomerID[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (mostFreqCustomerID[\"CustomerID\"], mostFreqCustomerID[\"Total\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION:\n",
    "\n",
    "- We see that most transactions happened in the UK, 495,478 out of 541,909 or around 91%.\n",
    "- It looks like we have big customers, like the most repeated customer in 135,080 transactions.\n",
    "\n",
    "Also, an important observation is that the descriptions of the products are not unified and thus it would be hard to explore product-related questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. A Filtered dataset\n",
    "\n",
    "Here we filter the dataset from the observations drawn in this profiling step. This allows us to have clean data to use in our analysis when we will analyse customers information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first remove the null values from the customer column:\n",
    "ecomDF0=ecomDF1.where(col(\"CustomerID\").isNotNull())\n",
    "\n",
    "#We also remove the null values from the description column:\n",
    "ecomDF0=ecomDF0.where(col(\"Description\").isNotNull())\n",
    "\n",
    "#We remove the negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this basic profiling, and with a good understanding of our data, we can now jump into analysing and try answering some business questions relevant to this e-commerce!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Getting some Insights to make the e-commerce grow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Revenues and number of orders, how good are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenues for the period were:\n",
      "+--------------+\n",
      "|Total Revenues|\n",
      "+--------------+\n",
      "|     9747748.0|\n",
      "+--------------+\n",
      "\n",
      "And this could be broken down to revenues by country:\n",
      "+--------------------+-------------------------+-------------------+\n",
      "|             Country|Total Revenues by Country|% of Total Revenues|\n",
      "+--------------------+-------------------------+-------------------+\n",
      "|      United Kingdom|                8187806.0|             83.997|\n",
      "|         Netherlands|                 284662.0|               2.92|\n",
      "|                EIRE|                 263277.0|              2.701|\n",
      "|             Germany|                 221698.0|              2.274|\n",
      "|              France|                 197404.0|              2.025|\n",
      "|           Australia|                 137077.0|              1.406|\n",
      "|         Switzerland|                  56385.0|              0.578|\n",
      "|               Spain|                  54775.0|              0.562|\n",
      "|             Belgium|                  40911.0|               0.42|\n",
      "|              Sweden|                  36596.0|              0.375|\n",
      "|               Japan|                  35341.0|              0.363|\n",
      "|              Norway|                  35163.0|              0.361|\n",
      "|            Portugal|                  29367.0|              0.301|\n",
      "|             Finland|                  22327.0|              0.229|\n",
      "|     Channel Islands|                  20086.0|              0.206|\n",
      "|             Denmark|                  18768.0|              0.193|\n",
      "|               Italy|                  16891.0|              0.173|\n",
      "|              Cyprus|                  12946.0|              0.133|\n",
      "|             Austria|                  10154.0|              0.104|\n",
      "|           Hong Kong|                  10117.0|              0.104|\n",
      "|           Singapore|                   9120.0|              0.094|\n",
      "|              Israel|                   7908.0|              0.081|\n",
      "|              Poland|                   7213.0|              0.074|\n",
      "|         Unspecified|                   4750.0|              0.049|\n",
      "|              Greece|                   4711.0|              0.048|\n",
      "|             Iceland|                   4310.0|              0.044|\n",
      "|              Canada|                   3666.0|              0.038|\n",
      "|               Malta|                   2505.0|              0.026|\n",
      "|United Arab Emirates|                   1902.0|               0.02|\n",
      "|                 USA|                   1731.0|              0.018|\n",
      "|             Lebanon|                   1694.0|              0.017|\n",
      "|           Lithuania|                   1661.0|              0.017|\n",
      "|  European Community|                   1292.0|              0.013|\n",
      "|              Brazil|                   1144.0|              0.012|\n",
      "|                 RSA|                   1002.0|               0.01|\n",
      "|      Czech Republic|                    708.0|              0.007|\n",
      "|             Bahrain|                    548.0|              0.006|\n",
      "|        Saudi Arabia|                    131.0|              0.001|\n",
      "+--------------------+-------------------------+-------------------+\n",
      "\n",
      "The total number of orders were:\n",
      "+------------+\n",
      "|Total Orders|\n",
      "+------------+\n",
      "|       25900|\n",
      "+------------+\n",
      "\n",
      "And this could be broken down to orders by country:\n",
      "+--------------------+-----------------------+-----------------+\n",
      "|             Country|Total Orders by Country|% of Total Orders|\n",
      "+--------------------+-----------------------+-----------------+\n",
      "|      United Kingdom|                  23494|            90.71|\n",
      "|             Germany|                    603|            2.328|\n",
      "|              France|                    461|             1.78|\n",
      "|                EIRE|                    360|             1.39|\n",
      "|             Belgium|                    119|            0.459|\n",
      "|               Spain|                    105|            0.405|\n",
      "|         Netherlands|                    101|             0.39|\n",
      "|         Switzerland|                     74|            0.286|\n",
      "|            Portugal|                     71|            0.274|\n",
      "|           Australia|                     69|            0.266|\n",
      "|               Italy|                     55|            0.212|\n",
      "|             Finland|                     48|            0.185|\n",
      "|              Sweden|                     46|            0.178|\n",
      "|              Norway|                     40|            0.154|\n",
      "|     Channel Islands|                     33|            0.127|\n",
      "|               Japan|                     28|            0.108|\n",
      "|              Poland|                     24|            0.093|\n",
      "|             Denmark|                     21|            0.081|\n",
      "|              Cyprus|                     20|            0.077|\n",
      "|             Austria|                     19|            0.073|\n",
      "|           Hong Kong|                     15|            0.058|\n",
      "|         Unspecified|                     13|             0.05|\n",
      "|           Singapore|                     10|            0.039|\n",
      "|               Malta|                     10|            0.039|\n",
      "|              Israel|                      9|            0.035|\n",
      "|             Iceland|                      7|            0.027|\n",
      "|                 USA|                      7|            0.027|\n",
      "|              Canada|                      6|            0.023|\n",
      "|              Greece|                      6|            0.023|\n",
      "|  European Community|                      5|            0.019|\n",
      "|      Czech Republic|                      5|            0.019|\n",
      "|             Bahrain|                      4|            0.015|\n",
      "|           Lithuania|                      4|            0.015|\n",
      "|United Arab Emirates|                      3|            0.012|\n",
      "|        Saudi Arabia|                      2|            0.008|\n",
      "|                 RSA|                      1|            0.004|\n",
      "|              Brazil|                      1|            0.004|\n",
      "|             Lebanon|                      1|            0.004|\n",
      "+--------------------+-----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round, countDistinct\n",
    "\n",
    "ecomDF2=ecomDF1.withColumn(\"Total_Spent\", col(\"UnitPrice\")*col(\"Quantity\"))\n",
    "\n",
    "print(\"Total revenues for the period were:\")\n",
    "totRev=ecomDF2.select(round(sum(\"Total_Spent\")).alias(\"Total Revenues\"))\n",
    "totRev.show()\n",
    "\n",
    "print(\"And this could be broken down to revenues by country:\")\n",
    "revByCountry=ecomDF2.groupBy(\"Country\")\\\n",
    "        .agg(sum(\"Total_spent\"))\n",
    "\n",
    "revByCountry=revByCountry.withColumn(\"PercentageOfTotRevenues\", col(\"sum(Total_spent)\")*100/totRev.collect()[0][0])\n",
    "revByCountry=revByCountry.sort(col(\"sum(Total_spent)\").desc())\n",
    "revByCountry.select(\"Country\",round(\"sum(Total_spent)\").alias(\"Total Revenues by Country\"), round(\"PercentageOfTotRevenues\",3).alias(\"% of Total Revenues\")).show(50)\n",
    "\n",
    "\n",
    "print(\"The total number of orders were:\")\n",
    "ecomDF1.select(countDistinct(\"InvoiceNo\").alias(\"Total Orders\")).show()\n",
    "\n",
    "print(\"And this could be broken down to orders by country:\")\n",
    "ordersByCountry=ecomDF1.groupBy(\"Country\")\\\n",
    "        .agg(countDistinct(\"InvoiceNo\"))\n",
    "\n",
    "ordersByCountry=ordersByCountry.withColumn(\"PercentageOfTotOrders\", col(\"count(DISTINCT InvoiceNo)\")*100/25900)\n",
    "ordersByCountry=ordersByCountry.sort(col(\"count(DISTINCT InvoiceNo)\").desc())\n",
    "ordersByCountry.select(\"Country\",col(\"count(DISTINCT InvoiceNo)\").alias(\"Total Orders by Country\"), round(\"PercentageOfTotOrders\",3).alias(\"% of Total Orders\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cancelled items are:\n",
      "+--------------------------------+\n",
      "|Number of cancelled transactions|\n",
      "+--------------------------------+\n",
      "|                            9288|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, count\n",
    "cancel=ecomDF.withColumn('can', concat(ecomDF.InvoiceNo.substr(0,1)))\n",
    "\n",
    "cancel=cancel.where(col(\"can\")==\"C\")\n",
    "\n",
    "print(\"The number of cancelled items are:\")\n",
    "cancel.select(count(col(\"can\")).alias(\"Number of cancelled transactions\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. When do people shop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin hours of day into 6 categories and find most amount of traffic + percentages:\n",
      "+---------------+---------------+------------+\n",
      "|         Period|NumTransactions|RoundedRatio|\n",
      "+---------------+---------------+------------+\n",
      "|Early afternoon|         295958|       54.61|\n",
      "|        Morning|         149952|       27.67|\n",
      "| Late afternoon|          94704|       17.48|\n",
      "|          Night|            871|        0.16|\n",
      "|  Early morning|            424|        0.08|\n",
      "+---------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Bin hours of day into 6 categories and find most amount of traffic + percentages:\")\n",
    "split_col = split(ecomDF1['Time'], ':')\n",
    "hours = ecomDF1.withColumn('Timing', split_col.getItem(0))\n",
    "\n",
    "\n",
    "hoursDF = hours.withColumn(\"TimeH\", hours[\"Timing\"].cast(IntegerType()))\n",
    "\n",
    "totalTr = hoursDF.count()\n",
    "hoursCategorizationDF = hoursDF\\\n",
    "   .withColumn(\"Period\", when((col(\"TimeH\")>=0) & (col(\"TimeH\")<4),\"Very late night\")\\\n",
    "                               .when((col(\"TimeH\")>=4) & (col(\"TimeH\")<8),\"Early morning\")\\\n",
    "                               .when((col(\"TimeH\")>=8) & (col(\"TimeH\")<12),\"Morning\")\\\n",
    "                               .when((col(\"TimeH\")>=12) & (col(\"TimeH\")<16),\"Early afternoon\")\\\n",
    "                               .when((col(\"TimeH\")>=16) & (col(\"TimeH\")<20),\"Late afternoon\")\\\n",
    "                               .otherwise(\"Night\"))\n",
    "\n",
    "hoursCategorizationDF.select(\"Period\")\\\n",
    "                     .groupBy(\"Period\")\\\n",
    "                     .agg(count(\"Period\").alias(\"NumTransactions\"), \\\n",
    "                          (count(\"Period\")/totalTr*100).alias(\"Ratio\"))\\\n",
    "                     .sort(col(\"NumTransactions\").desc())\\\n",
    "                     .select(\"Period\",\"NumTransactions\",round(\"Ratio\",2).alias(\"RoundedRatio\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenues broken down by month:\n",
      "+-----+-----------------------+-------------------+\n",
      "|Month|Total Revenues by Month|% of Total Revenues|\n",
      "+-----+-----------------------+-------------------+\n",
      "|   11|              1461756.0|             14.996|\n",
      "|   12|              1182625.0|             12.132|\n",
      "|   10|              1070705.0|             10.984|\n",
      "|    9|              1019688.0|             10.461|\n",
      "|    5|               723334.0|              7.421|\n",
      "|    6|               691123.0|               7.09|\n",
      "|    3|               683267.0|              7.009|\n",
      "|    8|               682681.0|              7.003|\n",
      "|    7|               681300.0|              6.989|\n",
      "|    1|               560000.0|              5.745|\n",
      "|    2|               498063.0|               5.11|\n",
      "|    4|               493207.0|               5.06|\n",
      "+-----+-----------------------+-------------------+\n",
      "\n",
      "Orders broken down by month:\n",
      "+-----+----------------------+-----------------+\n",
      "|month|Total Orders per month|% of Total Orders|\n",
      "+-----+----------------------+-----------------+\n",
      "|   11|                  3462|           13.367|\n",
      "|   12|                  3040|           11.737|\n",
      "|   10|                  2637|           10.181|\n",
      "|    9|                  2327|            8.985|\n",
      "|    5|                  2162|            8.347|\n",
      "|    6|                  2012|            7.768|\n",
      "|    3|                  1983|            7.656|\n",
      "|    7|                  1927|             7.44|\n",
      "|    4|                  1744|            6.734|\n",
      "|    8|                  1737|            6.707|\n",
      "|    1|                  1476|            5.699|\n",
      "|    2|                  1393|            5.378|\n",
      "+-----+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "ecomDF2=ecomDF1.withColumn(\"Total_Spent\", col(\"UnitPrice\")*col(\"Quantity\"))\n",
    "\n",
    "totRev=ecomDF2.select(round(sum(\"Total_Spent\")).alias(\"Total Revenues\"))\n",
    "\n",
    "print(\"Revenues broken down by month:\")\n",
    "revBymonth=ecomDF2.groupBy(\"month\")\\\n",
    "        .agg(sum(\"Total_spent\"))\n",
    "\n",
    "revBymonth=revBymonth.withColumn(\"PercentageOfTotRevenues\", col(\"sum(Total_spent)\")*100/totRev.collect()[0][0])\n",
    "revBymonth=revBymonth.sort(col(\"sum(Total_spent)\").desc())\n",
    "revBymonth.select(\"Month\",round(\"sum(Total_spent)\").alias(\"Total Revenues by Month\"), round(\"PercentageOfTotRevenues\",3).alias(\"% of Total Revenues\")).show()\n",
    "\n",
    "print(\"Orders broken down by month:\")\n",
    "ordersBymonth=ecomDF1.groupBy(\"month\")\\\n",
    "        .agg(countDistinct(\"InvoiceNo\"))\n",
    "\n",
    "ordersBymonth=ordersBymonth.withColumn(\"PercentageOfTotOrders\", col(\"count(DISTINCT InvoiceNo)\")*100/25900)\n",
    "ordersBymonth=ordersBymonth.sort(col(\"count(DISTINCT InvoiceNo)\").desc())\n",
    "ordersBymonth.select(\"month\",col(\"count(DISTINCT InvoiceNo)\").alias(\"Total Orders per month\"), round(\"PercentageOfTotOrders\",3).alias(\"% of Total Orders\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenues broken down by day of the week:\n",
      "+----------+---------------------------------+-------------------+\n",
      "|DayofWeekS|Total Revenues by day of the week|% of Total Revenues|\n",
      "+----------+---------------------------------+-------------------+\n",
      "|       Thu|                        2112519.0|             21.672|\n",
      "|       Tue|                        1966183.0|             20.171|\n",
      "|       Wed|                        1734147.0|              17.79|\n",
      "|       Mon|                        1588609.0|             16.297|\n",
      "|       Fri|                        1540611.0|             15.805|\n",
      "|       Sun|                         805679.0|              8.265|\n",
      "+----------+---------------------------------+-------------------+\n",
      "\n",
      "Orders broken down by day of the week:\n",
      "+----------+----------------------+-----------------+\n",
      "|DayofWeekS|Total Orders per month|% of Total Orders|\n",
      "+----------+----------------------+-----------------+\n",
      "|       Thu|                  5660|           21.853|\n",
      "|       Wed|                  4815|           18.591|\n",
      "|       Tue|                  4722|           18.232|\n",
      "|       Fri|                  4184|           16.154|\n",
      "|       Mon|                  4138|           15.977|\n",
      "|       Sun|                  2381|            9.193|\n",
      "+----------+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "ecomDF2=ecomDF1.withColumn(\"Total_Spent\", col(\"UnitPrice\")*col(\"Quantity\"))\n",
    "\n",
    "totRev=ecomDF2.select(round(sum(\"Total_Spent\")).alias(\"Total Revenues\"))\n",
    "\n",
    "print(\"Revenues broken down by day of the week:\")\n",
    "revByCountry=ecomDF2.groupBy(\"DayofWeekS\")\\\n",
    "        .agg(sum(\"Total_spent\"))\n",
    "\n",
    "revByCountry=revByCountry.withColumn(\"PercentageOfTotRevenues\", col(\"sum(Total_spent)\")*100/totRev.collect()[0][0])\n",
    "revByCountry=revByCountry.sort(col(\"sum(Total_spent)\").desc())\n",
    "revByCountry.select(\"DayofWeekS\",round(\"sum(Total_spent)\").alias(\"Total Revenues by day of the week\"), round(\"PercentageOfTotRevenues\",3).alias(\"% of Total Revenues\")).show()\n",
    "\n",
    "print(\"Orders broken down by day of the week:\")\n",
    "ordersByDOW=ecomDF1.groupBy(\"DayofWeekS\")\\\n",
    "        .agg(countDistinct(\"InvoiceNo\"))\n",
    "\n",
    "ordersByDOW=ordersByDOW.withColumn(\"PercentageOfTotOrders\", col(\"count(DISTINCT InvoiceNo)\")*100/25900)\n",
    "ordersByDOW=ordersByDOW.sort(col(\"count(DISTINCT InvoiceNo)\").desc())\n",
    "ordersByDOW.select(\"DayofWeekS\",col(\"count(DISTINCT InvoiceNo)\").alias(\"Total Orders by day of the week\"), round(\"PercentageOfTotOrders\",3).alias(\"% of Total Orders\")).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. What does customers analysis  show? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenues from the customers of whom we have the IDs:\n",
      "+--------------+\n",
      "|Total Revenues|\n",
      "+--------------+\n",
      "|     8300066.0|\n",
      "+--------------+\n",
      "\n",
      "The average amount spent by customers for an order is: 374.0453357368184 Pounds.\n",
      " \n",
      "The total number of customers of the business is: \n",
      "+---------------+\n",
      "|Total Customers|\n",
      "+---------------+\n",
      "|           4372|\n",
      "+---------------+\n",
      "\n",
      "The average amount of orders by customers is: 5.07548032936871 orders.\n",
      " \n",
      "Who are the most valuable customers, where are they from and how much they have spent:\n",
      "+--------------+----------+--------------------------+-------------------+\n",
      "|       Country|CustomerID|Total Revenues by Customer|% of Total Revenues|\n",
      "+--------------+----------+--------------------------+-------------------+\n",
      "|   Netherlands|     14646|                  279489.0|              3.367|\n",
      "|United Kingdom|     18102|                  256438.0|               3.09|\n",
      "|United Kingdom|     17450|                  187482.0|              2.259|\n",
      "|          EIRE|     14911|                  132573.0|              1.597|\n",
      "|     Australia|     12415|                  123725.0|              1.491|\n",
      "|          EIRE|     14156|                  113384.0|              1.366|\n",
      "|United Kingdom|     17511|                   88125.0|              1.062|\n",
      "|United Kingdom|     16684|                   65892.0|              0.794|\n",
      "|United Kingdom|     13694|                   62653.0|              0.755|\n",
      "|United Kingdom|     15311|                   59419.0|              0.716|\n",
      "|United Kingdom|     13089|                   57386.0|              0.691|\n",
      "|United Kingdom|     14096|                   57121.0|              0.688|\n",
      "|United Kingdom|     15061|                   54229.0|              0.653|\n",
      "|United Kingdom|     17949|                   52751.0|              0.636|\n",
      "|United Kingdom|     15769|                   51824.0|              0.624|\n",
      "|United Kingdom|     16029|                   50993.0|              0.614|\n",
      "|United Kingdom|     14298|                   50862.0|              0.613|\n",
      "|United Kingdom|     14088|                   50415.0|              0.607|\n",
      "|United Kingdom|     17841|                   40341.0|              0.486|\n",
      "|United Kingdom|     13798|                   36351.0|              0.438|\n",
      "+--------------+----------+--------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Orders broken down by customers:\n",
      "+----------+--------------------------+-----------------+\n",
      "|CustomerID|Total Orders by customers:|% of Total Orders|\n",
      "+----------+--------------------------+-----------------+\n",
      "|     14911|                       248|            0.958|\n",
      "|     12748|                       224|            0.865|\n",
      "|     17841|                       169|            0.653|\n",
      "|     14606|                       128|            0.494|\n",
      "|     15311|                       118|            0.456|\n",
      "|     13089|                       118|            0.456|\n",
      "|     12971|                        89|            0.344|\n",
      "|     14527|                        86|            0.332|\n",
      "|     13408|                        81|            0.313|\n",
      "|     14646|                        77|            0.297|\n",
      "|     16029|                        76|            0.293|\n",
      "|     16422|                        75|             0.29|\n",
      "|     14156|                        66|            0.255|\n",
      "|     13798|                        63|            0.243|\n",
      "|     18102|                        62|            0.239|\n",
      "|     13694|                        60|            0.232|\n",
      "|     17450|                        55|            0.212|\n",
      "|     15061|                        55|            0.212|\n",
      "|     16013|                        54|            0.208|\n",
      "|     15189|                        53|            0.205|\n",
      "+----------+--------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate customer lifetime values: amount earned over time \n",
    "from pyspark.sql.functions import sum, round, countDistinct\n",
    "\n",
    "ecomDF5=ecomDF0.withColumn(\"Total_Spent\", col(\"UnitPrice\")*col(\"Quantity\"))\n",
    "\n",
    "print(\"Total Revenues from the customers of whom we have the IDs:\")\n",
    "totRev=ecomDF5.select(round(sum(\"Total_Spent\")).alias(\"Total Revenues\"))\n",
    "totRev.show()\n",
    "\n",
    "totOrd=ecomDF5.select(countDistinct(\"InvoiceNo\").alias(\"Total Orders\"))\n",
    "\n",
    "#get the average amount of money spent by transaction by customers\n",
    "print(\"The average amount spent by customers for an order is: \" + str(totRev.collect()[0][0]/totOrd.collect()[0][0]) + \" Pounds.\")\n",
    "\n",
    "totCusto=ecomDF5.select(countDistinct(\"CustomerID\").alias(\"Total Customers\"))\n",
    "print(\" \")\n",
    "print(\"The total number of customers of the business is: \")\n",
    "totCusto.show()\n",
    "\n",
    "print(\"The average amount of orders by customers is: \" + str(totOrd.collect()[0][0]/totCusto.collect()[0][0]) + \" orders.\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Who are the most valuable customers, where are they from and how much they have spent:\")\n",
    "revByCountry=ecomDF5.groupBy(\"CustomerID\", \"Country\")\\\n",
    "        .agg(sum(\"Total_spent\"))\n",
    "\n",
    "revByCountry=revByCountry.withColumn(\"PercentageOfTotRevenues\", col(\"sum(Total_spent)\")*100/totRev.collect()[0][0])\n",
    "revByCountry=revByCountry.sort(col(\"sum(Total_spent)\").desc())\n",
    "revByCountry.select(\"Country\",\"CustomerID\",round(\"sum(Total_spent)\").alias(\"Total Revenues by Customer\"), round(\"PercentageOfTotRevenues\",3).alias(\"% of Total Revenues\")).show()\n",
    "\n",
    "print(\"Orders broken down by customers:\")\n",
    "ordersByCusto=ecomDF5.groupBy(\"CustomerID\")\\\n",
    "        .agg(countDistinct(\"InvoiceNo\"))\n",
    "\n",
    "ordersByCusto=ordersByCusto.withColumn(\"PercentageOfTotOrders\", col(\"count(DISTINCT InvoiceNo)\")*100/25900)\n",
    "ordersByCusto=ordersByCusto.sort(col(\"count(DISTINCT InvoiceNo)\").desc())\n",
    "ordersByCusto.select(\"CustomerID\",col(\"count(DISTINCT InvoiceNo)\").alias(\"Total Orders by customers:\"), round(\"PercentageOfTotOrders\",3).alias(\"% of Total Orders\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders broken down by customers:\n",
      "+----------+-----------------+-----------------------+\n",
      "|Occurences|count(Occurences)|Percentage of customers|\n",
      "+----------+-----------------+-----------------------+\n",
      "|         1|             1313|                   30.0|\n",
      "|         2|              817|                   19.0|\n",
      "|         3|              490|                   11.0|\n",
      "|         4|              377|                    9.0|\n",
      "|         5|              288|                    7.0|\n",
      "|         6|              196|                    4.0|\n",
      "|         7|              157|                    4.0|\n",
      "|         8|              117|                    3.0|\n",
      "|         9|               80|                    2.0|\n",
      "|        10|               78|                    2.0|\n",
      "|        11|               62|                    1.0|\n",
      "|        12|               51|                    1.0|\n",
      "|        13|               38|                    1.0|\n",
      "|        14|               41|                    1.0|\n",
      "|        15|               28|                    1.0|\n",
      "|        16|               26|                    1.0|\n",
      "|        17|               19|                    0.0|\n",
      "|        18|               22|                    1.0|\n",
      "|        19|               16|                    0.0|\n",
      "|        20|               14|                    0.0|\n",
      "+----------+-----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "So we see that almost 60% of customers only bought three times or less.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round, countDistinct\n",
    "print(\"Orders broken down by customers:\")\n",
    "ordersByCusto=ecomDF5.groupBy(\"CustomerID\")\\\n",
    "        .agg(countDistinct(\"InvoiceNo\"))\\\n",
    "        .withColumn(\"PercentageOfTotOrders\", col(\"count(DISTINCT InvoiceNo)\")*100/25900)\\\n",
    "        .sort(col(\"count(DISTINCT InvoiceNo)\").desc())\\\n",
    "        .select(\"CustomerID\",col(\"count(DISTINCT InvoiceNo)\").alias(\"Occurences\"))\\\n",
    "        .groupBy(\"Occurences\")\\\n",
    "        .agg(count(\"Occurences\"))\\\n",
    "        .sort(col(\"Occurences\").alias(\"Occurences\").asc())\n",
    "ordersByCusto.withColumn(\"Percentage of customers\",round(col(\"count(Occurences)\")*100/4372)).show()\n",
    "\n",
    "print(\"So we see that almost 60% of customers only bought three times or less.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
